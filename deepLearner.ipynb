{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building an Artificial Neural Network\n",
    "\n",
    "#import libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "ds = pd.read_csv(\"Churn_Modelling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RowNumber  CustomerId   Surname  CreditScore  Gender  Age  Tenure  \\\n",
      "0          1    15634602  Hargrave          619       0   42       2   \n",
      "1          2    15647311      Hill          608       0   41       1   \n",
      "2          3    15619304      Onio          502       0   42       8   \n",
      "\n",
      "     Balance  NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \\\n",
      "0       0.00              1          1               1        101348.88   \n",
      "1   83807.86              1          0               1        112542.58   \n",
      "2  159660.80              3          1               0        113931.57   \n",
      "\n",
      "   Exited  Geography_France  Geography_Germany  Geography_Spain  \n",
      "0       1                 1                  0                0  \n",
      "1       0                 0                  0                1  \n",
      "2       1                 1                  0                0  \n"
     ]
    }
   ],
   "source": [
    "#label encode 'gender' feature\n",
    "lb_encode = preprocessing.LabelEncoder()\n",
    "ds['Gender'] = lb_encode.fit_transform(ds['Gender'])\n",
    "\n",
    "#one hot encode 'Geography' feature\n",
    "ds = pd.get_dummies(ds, columns = ['Geography'])\n",
    "\n",
    "print(ds.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619.   0.  42. ...   1.   1.   0.]\n",
      " [608.   0.  41. ...   0.   0.   0.]\n",
      " [502.   0.  42. ...   1.   1.   0.]\n",
      " ...\n",
      " [709.   0.  36. ...   1.   1.   0.]\n",
      " [772.   1.  42. ...   1.   0.   1.]\n",
      " [792.   0.  28. ...   0.   1.   0.]]\n",
      "[0 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#create dependant and independant variables\n",
    "x = ds.iloc[:,3:-1].values\n",
    "y = ds.iloc[:,-1].values\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature scaling: compulsory for deep learning.\n",
    "#feature scaling ALL features\n",
    "\n",
    "sc = preprocessing.StandardScaler()\n",
    "\n",
    "#scale both variables seperately, to prevent information leak.\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization of ANN\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "#add input layer\n",
    "\n",
    "#creating 6 neurons\n",
    "# #setting the activation function as the rectifier linear functions\n",
    "layerOne = tf.keras.layers.Dense(units=6, activation='relu')\n",
    "\n",
    "#add first layer to our ANN\n",
    "ann.add(layerOne)\n",
    "\n",
    "#second layer\n",
    "layerTwo = tf.keras.layers.Dense(units=6, activation='relu')\n",
    "\n",
    "#add second layer to our ANN\n",
    "ann.add(layerTwo)\n",
    "\n",
    "#output layer\n",
    "layerOut = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "#add output layer to our ANN\n",
    "ann.add(layerOut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "250/250 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7379\n",
      "Epoch 2/100\n",
      "250/250 [==============================] - 0s 975us/step - loss: 0.3250 - accuracy: 0.8300\n",
      "Epoch 3/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9872\n",
      "Epoch 4/100\n",
      "250/250 [==============================] - 0s 968us/step - loss: 0.0302 - accuracy: 0.9999\n",
      "Epoch 5/100\n",
      "250/250 [==============================] - 0s 979us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "250/250 [==============================] - 0s 913us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "250/250 [==============================] - 0s 956us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "250/250 [==============================] - 0s 934us/step - loss: 7.9010e-04 - accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 5.7566e-04 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 4.3241e-04 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "250/250 [==============================] - 0s 963us/step - loss: 3.3267e-04 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.6070e-04 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.0732e-04 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "250/250 [==============================] - 0s 988us/step - loss: 1.6691e-04 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "250/250 [==============================] - 0s 961us/step - loss: 1.3575e-04 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "250/250 [==============================] - 0s 932us/step - loss: 1.1133e-04 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "250/250 [==============================] - 0s 991us/step - loss: 9.1951e-05 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 7.6407e-05 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "250/250 [==============================] - 0s 932us/step - loss: 6.3826e-05 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "250/250 [==============================] - 0s 929us/step - loss: 5.3543e-05 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "250/250 [==============================] - 0s 938us/step - loss: 4.5088e-05 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "250/250 [==============================] - 0s 939us/step - loss: 3.8101e-05 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 3.2286e-05 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "250/250 [==============================] - 0s 962us/step - loss: 2.7430e-05 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "250/250 [==============================] - 0s 968us/step - loss: 2.3356e-05 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "250/250 [==============================] - 0s 969us/step - loss: 1.9926e-05 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.7032e-05 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.4581e-05 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.2502e-05 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.0732e-05 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 9.2231e-06 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 7.9352e-06 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 6.8339e-06 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "250/250 [==============================] - 0s 974us/step - loss: 5.8915e-06 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 5.0825e-06 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "250/250 [==============================] - 0s 978us/step - loss: 4.3878e-06 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 3.7907e-06 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 3.2768e-06 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.8340e-06 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "250/250 [==============================] - 0s 979us/step - loss: 2.4528e-06 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "250/250 [==============================] - 0s 948us/step - loss: 2.1235e-06 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.8394e-06 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.5940e-06 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.3817e-06 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.1982e-06 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.0393e-06 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 9.0194e-07 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "250/250 [==============================] - 0s 937us/step - loss: 7.8297e-07 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "250/250 [==============================] - 0s 943us/step - loss: 6.7984e-07 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "250/250 [==============================] - 0s 964us/step - loss: 5.9060e-07 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 5.1311e-07 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "250/250 [==============================] - 0s 994us/step - loss: 4.4594e-07 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "250/250 [==============================] - 0s 966us/step - loss: 3.8768e-07 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "250/250 [==============================] - 0s 951us/step - loss: 3.3712e-07 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "250/250 [==============================] - 0s 981us/step - loss: 2.9321e-07 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.5509e-07 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "250/250 [==============================] - 0s 948us/step - loss: 2.2198e-07 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.9321e-07 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "250/250 [==============================] - 0s 970us/step - loss: 1.6819e-07 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.4648e-07 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "250/250 [==============================] - 0s 976us/step - loss: 1.2759e-07 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "250/250 [==============================] - 0s 988us/step - loss: 1.1114e-07 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 9.6874e-08 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 8.4463e-08 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 7.3638e-08 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 6.4222e-08 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 5.6019e-08 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "250/250 [==============================] - 0s 908us/step - loss: 4.8864e-08 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "250/250 [==============================] - 0s 985us/step - loss: 4.2621e-08 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "250/250 [==============================] - 0s 930us/step - loss: 3.7183e-08 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "250/250 [==============================] - 0s 983us/step - loss: 3.2457e-08 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "250/250 [==============================] - 0s 999us/step - loss: 2.8387e-08 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.4899e-08 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 2.1938e-08 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "250/250 [==============================] - 0s 974us/step - loss: 1.9427e-08 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.7285e-08 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.5474e-08 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "250/250 [==============================] - 0s 946us/step - loss: 1.3918e-08 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "250/250 [==============================] - 0s 968us/step - loss: 1.2586e-08 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "250/250 [==============================] - 0s 978us/step - loss: 1.1375e-08 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 1.0340e-08 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "250/250 [==============================] - 0s 934us/step - loss: 9.4540e-09 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "250/250 [==============================] - 0s 962us/step - loss: 8.6640e-09 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "250/250 [==============================] - 0s 950us/step - loss: 7.9683e-09 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "250/250 [==============================] - 0s 936us/step - loss: 7.3676e-09 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 6.8166e-09 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "250/250 [==============================] - 0s 948us/step - loss: 6.3215e-09 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "250/250 [==============================] - 0s 989us/step - loss: 5.9136e-09 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "250/250 [==============================] - 0s 924us/step - loss: 5.5785e-09 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 5.2353e-09 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 4.9367e-09 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "250/250 [==============================] - 0s 954us/step - loss: 4.6794e-09 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "250/250 [==============================] - 0s 985us/step - loss: 4.4585e-09 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "250/250 [==============================] - 0s 948us/step - loss: 4.2851e-09 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 4.1347e-09 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "250/250 [==============================] - 0s 937us/step - loss: 3.9485e-09 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "250/250 [==============================] - 0s 961us/step - loss: 3.7875e-09 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "250/250 [==============================] - 0s 1000us/step - loss: 3.6363e-09 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e1bf643100>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the ANN\n",
    "\n",
    "#compile ANN\n",
    "ann.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "#training ANN on training set\n",
    "ann.fit(x_train, y_train, batch_size= 32, epochs= 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[False]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.predict(sc.transform([[1,0,0,600,1,40,3,60000,2,1,1,50000]])) > 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6246b25e200e4c5124e3e61789ac81350562f0761bbcf92ad9e48654207659c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
